{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(df, target, standardize, thr_bins = [-np.inf, 0.01, 0.52, 6.57, 1090.84], model_name='ridgeRegression', skew=False, plot = False):\n",
    "    print(\"model: \", model_name)\n",
    "    # preprocessing pipline, no shuffling,\n",
    "    X, y = preproc_df(df, target, standardize, skew)\n",
    "\n",
    "    if model_name == 'randomForest':\n",
    "        model = RandomForestRegressor()\n",
    "    elif model_name == 'ridgeRegression':\n",
    "        model = Ridge()\n",
    "    elif model_name == 'MLPRegressor':\n",
    "        model = MLPRegressor()\n",
    "    \n",
    "    \n",
    "    # creating bins for target, startified sampling\n",
    "    bins = pd.cut(y, bins=thr_bins, labels=[1, 2, 3, 4])\n",
    "    # stratified sampling needed for each fold\n",
    "    # .split(X,y_labels) method for StratifiedKFold: \"Generate indices to split data into training and test set.\"\n",
    "    # is normally used for classification task with classes as target, but since target is binned \n",
    "    # .split() could be used for stratified sampling\n",
    "    strat_kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=35007).split(X, bins)\n",
    "    # what about hyperparameter tuning within each fold?\n",
    "    \n",
    "    if not(plot):\n",
    "        scores = cross_val_score(model, X, y, cv=strat_kf, scoring='neg_mean_squared_error')\n",
    "        # RMSE, negate because scoring neg_mean_sqared_error ouput negative MeanSquaredError\n",
    "        rmse_scores = np.sqrt(-scores)\n",
    "        # model should be returned too ! for testing\n",
    "        return rmse_scores\n",
    "    else:\n",
    "        train_sizes, train_scores, test_scores = learning_curve(\n",
    "            model, X, y, cv=strat_kf, scoring='neg_mean_squared_error', \n",
    "            train_sizes=np.linspace(0.1, 1.0, 10)\n",
    "        )\n",
    "        # train_sizes=np.linspace(0.1, 1.0, 10): validation scores for 10% to 100% of the training data (increasing size of the training set by 10%)\n",
    "        \n",
    "        train_scores_mean = np.mean(np.sqrt(-train_scores), axis=1)\n",
    "        test_scores_mean = np.mean(np.sqrt(-test_scores), axis=1)\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.title(\"Learning Curve\")\n",
    "        plt.xlabel(\"Size of Training Set\")\n",
    "        plt.ylabel(\"Mean RSME Score\")\n",
    "        plt.grid()\n",
    "        \n",
    "        # x-axis actual size of the training set (10%-100%)\n",
    "        # y-axis the RMSE \n",
    "        plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "        plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "        \n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.show()\n",
    "        return np.sqrt(-train_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_name, ridge_alpha=1.0, ridge_fit_intercept=True):\n",
    "    if model_name == 'randomForest':\n",
    "        return RandomForestRegressor()\n",
    "    elif model_name == 'ridgeRegression':\n",
    "        print('ridgeRegression')\n",
    "        return Ridge(alpha=ridge_alpha, fit_intercept=ridge_fit_intercept)\n",
    "    elif model_name == 'MLPRegressor':\n",
    "        return MLPRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model2(df, target, model, standardize, thr_bins = [-np.inf, 0.01, 0.52, 6.57, 1090.84], bin_label=[1,2,3,4], kfolds=5, skew=False, plot = False):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - df: DataFrame\n",
    "    - target: str, target variable.\n",
    "    - model: scikit-learn model (regression)\n",
    "    - standardize: bool, standardize (from preproc_df).\n",
    "    - thr_bins: list, thresholds for binning target variable.\n",
    "    - skew: bool, apply skewness correction for RH (from preproc_df, if standardize=False).\n",
    "    - plot: bool\n",
    "    \n",
    "    Returns:\n",
    "    (plot=False):\n",
    "        - RMSE scores \n",
    "    (plot=True):\n",
    "        - learning curve\n",
    "        - train_scores RSME scores from cross-validation\n",
    "    \"\"\"\n",
    "\n",
    "    # preprocessing pipline, no shuffling,\n",
    "    X, y = preproc_df(df, target, standardize, skew)\n",
    "\n",
    "   \n",
    "    # creating bins for target, startified sampling\n",
    "    bins = pd.cut(y, bins=thr_bins, labels=bin_label)\n",
    "    # stratified sampling needed for each fold\n",
    "    # .split(X,y_labels) method for StratifiedKFold: \"Generate indices to split data into training and test set.\"\n",
    "    # is normally used for classification task with classes as target, but since target is binned \n",
    "    # .split() could be used for stratified sampling\n",
    "    strat_kf = StratifiedKFold(n_splits=kfolds, shuffle=True, random_state=35007).split(X, bins)\n",
    "    # what about hyperparameter tuning within each fold?\n",
    "    \n",
    "    if not(plot):\n",
    "        scores = cross_val_score(model, X, y, cv=strat_kf, scoring='neg_mean_squared_error')\n",
    "        # RMSE, negate because scoring neg_mean_sqared_error ouput negative MeanSquaredError\n",
    "        rmse_scores = np.sqrt(-scores)\n",
    "        # model should be returned too ! for testing\n",
    "        return rmse_scores\n",
    "    else:\n",
    "        train_sizes, train_scores, test_scores = learning_curve(\n",
    "            model, X, y, cv=strat_kf, scoring='neg_mean_squared_error', \n",
    "            train_sizes=np.linspace(0.1, 1.0, 10)\n",
    "        )\n",
    "        # train_sizes=np.linspace(0.1, 1.0, 10): validation scores for 10% to 100% of the training data (increasing size of the training set by 10%)\n",
    "        \n",
    "        train_scores_mean = np.mean(np.sqrt(-train_scores), axis=1)\n",
    "        test_scores_mean = np.mean(np.sqrt(-test_scores), axis=1)\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.title(\"Learning Curve\")\n",
    "        plt.xlabel(\"Size of Training Set\")\n",
    "        plt.ylabel(\"Mean RSME Score\")\n",
    "        plt.grid()\n",
    "        \n",
    "        # x-axis actual size of the training set (10%-100%)\n",
    "        # y-axis the RMSE \n",
    "        plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "        plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "        \n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.show()\n",
    "        return np.sqrt(-train_scores)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
